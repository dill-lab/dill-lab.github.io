<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | DILL Lab </title> <meta name="author" content=" "> <meta name="description" content="... and preprints"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/dill-canva-transp.png?832fc9ec4362f2a00cfd9479ed52886f"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dill-lab.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> DILL Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/opportunities/">opportunities </a> </li> <li class="nav-item "> <a class="nav-link" href="/social/">social </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <img src="/assets/img/viterbi.png" alt="click here" height="58px" width="200px" style="float:right"> <h1 class="post-title">publications</h1> <p class="post-description">... and preprints</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="nazir2025betterlanguagemodelinversion" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2506.17090" target="_blank" rel="external nofollow noopener"><b>Better Language Model Inversion by Compactly Representing Next-Token Distributions</b></a></div> <div class="author"> Murtaza Nazir, <a href="https://mattf1n.github.io/" rel="external nofollow noopener" target="_blank"> <b> Matthew Finlayson </b> </a>, John X. Morris, Xiang Ren, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>Proc. of NeurIPS</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model’s system message. We propose a new method – prompt inversion from logprob sequences (PILS) – that recovers hidden prompts by gleaning clues from the model’s next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2–3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5–27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CoLM</abbr> </div> <div id="wang2025teachingmodelsunderstandbut" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2505.03052" target="_blank" rel="external nofollow noopener"><b>Teaching Models to Understand (but not Generate) High-risk Data</b></a></div> <div class="author"> <a href="https://ryanyxw.github.io/" rel="external nofollow noopener" target="_blank"> <b> Ryan Wang </b> </a>, <a href="https://mattf1n.github.io/" rel="external nofollow noopener" target="_blank"> <b> Matthew Finlayson </b> </a>, Luca Soldaini, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, and Robin Jia </div> <div class="periodical"> <em>Conference on Language Modeling</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> <a href="https://github.com/ryanyxw/llm-decouple" class="abstract btn btn-sm z-depth-1 rounded" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Language model developers typically filter out high-risk content – such as toxic or copyrighted text – from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models’ ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model’s context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models’ understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CoLM</abbr> </div> <div id="ghosh2025congrs" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2510.03527" target="_blank" rel="external nofollow noopener"><b>Sample, Align, Synthesize: Graph-Based Response Synthesis with CONGRS</b></a></div> <div class="author"> <a href="https://sghosh73.github.io" rel="external nofollow noopener" target="_blank"> <b> Sayan Ghosh </b> </a>, <a href="https://www.linkedin.com/in/shahzaib-saqib-warraich-348aa714a/" rel="external nofollow noopener" target="_blank"> <b> Shahzaib Saqib Warraich </b> </a>, Dhruv Tarsadiya, Gregory Yauney, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>COLM Workshop on Test-time Scaling and Reasoning Models (ScaLR@COLM) / NeurIPS Workshop on Efficient Reasoning</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> <a href="https://github.com/dill-lab/sample-fusion-with-congrs" class="abstract btn btn-sm z-depth-1 rounded" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Inference-time scaling methods improve language model performance, but existing methods lack the flexibility to synthesize information across multiple long-form generation samples. We introduce Consensus Graphs (CONGRS), a flexible DAG-based data structure that represents shared content and semantic variation across a set of LM responses to the same prompt. Constructing CONGRS relies on lightweight lexical multiple sequence alignment supplemented by targeted usage of a secondary LM judge, which reduces reliance on LM judges by more than 80% compared to other methods. Our experiments show that synthesizing responses from CONGRS improves factual precision on a biography generation task by up to 32% over an average response. We apply our approach to the MATH and AIME reasoning tasks and find an improvement over self-verification and majority vote baselines by up to 6 points of accuracy. CONGRS are a promising way to efficiently use the information provided by inference-time scaling.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="liu2025evaluationimperfectbenchmarksratings" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2504.09394" target="_blank" rel="external nofollow noopener"><b>Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification</b></a></div> <div class="author"> <a href="https://joseph.liu.us/" rel="external nofollow noopener" target="_blank"> <b> Joseph Liu </b> </a>, <a href="https://yoonsoonam.com/" rel="external nofollow noopener" target="_blank"> <b> Yoonsoo Nam </b> </a>, <a href="https://x-f-cui.github.io/website/" rel="external nofollow noopener" target="_blank"> <b> Xinyue Cui </b> </a>, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>NeurIPS Workshop on LLM Evaluation</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Despite the successes of language models, their evaluation remains a daunting challenge for new and existing tasks. We consider the task of text simplification, commonly used to improve information accessibility, where evaluation faces two major challenges. First, the data in existing benchmarks might not reflect the capabilities of current language models on the task, often containing disfluent, incoherent, or simplistic examples. Second, existing human ratings associated with the benchmarks often contain a high degree of disagreement, resulting in inconsistent ratings; nevertheless, existing metrics still have to show higher correlations with these imperfect ratings. As a result, evaluation for the task is not reliable and does not reflect expected trends (e.g., more powerful models being assigned higher scores). We address these challenges for the task of text simplification through three contributions. First, we introduce SynthSimpliEval, a synthetic benchmark for text simplification featuring simplified sentences generated by models of varying sizes. Through a pilot study, we show that human ratings on our benchmark exhibit high inter-annotator agreement and reflect the expected trend: larger models produce higher-quality simplifications. Second, we show that auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices to obtain consistent ratings for the evaluation of text simplification. Third, we demonstrate that existing learnable metrics for text simplification benefit from training on our LLMs-as-a-jury-rated synthetic data, closing the gap with pure LLMs-as-a-jury for evaluation. Overall, through our case study on text simplification, we show that a reliable evaluation requires higher quality test data, which could be obtained through synthetic data and LLMs-as-a-jury ratings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="ranjit2025nvdrs" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2508.18541" target="_blank" rel="external nofollow noopener"><b>Designing and Validating Intervention Opportunities for Suicide Prevention with Language Model Assistants</b></a></div> <div class="author"> <a href="https://jr4fs.github.io/" rel="external nofollow noopener" target="_blank"> <b> Jaspreet Ranjit </b> </a>, Hyundong J. Cho, Claire J. Smerdon, <a href="https://yoonsoonam.com/" rel="external nofollow noopener" target="_blank"> <b> Yoonsoo Nam </b> </a>, Myles Phung, Jonathan May, John R. Blosnich, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>EAAMO / NeurIPS Workshop on GenAI for Health</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> <a href="https://dill-lab.github.io/interventions_lm_assistants/" class="abstract btn btn-sm z-depth-1 rounded" role="button">Code</a> </div> <div class="abstract hidden"> <p>The National Violent Death Reporting System (NVDRS) documents information about suicides in the United States, including free text narratives (e.g., circumstances surrounding a suicide). In a demanding public health data pipeline, annotators manually extract structured information from death investigation records following extensive guidelines developed painstakingly by experts. In this work, we facilitate data-driven insights from the NVDRS data to support the development of novel suicide interventions by investigating the value of language models (LMs) as efficient assistants to these (a) data annotators and (b) experts. We find that LM predictions match existing data annotations about 85% of the time across 50 NVDRS variables. In the cases where the LM disagrees with existing annotations, expert review reveals that LM assistants can surface annotation discrepancies 38% of the time. Finally, we introduce a human-in-the-loop algorithm to assist experts in efficiently building and refining guidelines for annotating new variables by allowing them to focus only on providing feedback for incorrect LM predictions. We apply our algorithm to a real-world case study for a new variable that characterizes victim interactions with lawyers and demonstrate that it achieves comparable annotation quality with a laborious manual approach. Our findings provide evidence that LMs can serve as effective assistants to public health researchers who handle sensitive data in high-stakes scenarios.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="surana2025chemref" class="col-sm-8"> <div class="title"><a href="./" target="_blank"><b>ChEmREF: Evaluating Language Model Readiness for Chemical Emergency Response Assistance</b></a></div> <div class="author"> <a href="https://dill-lab.github.io/people/"> <b> Risha Surana </b> </a>, Qinyuan Ye, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>NeurIPS Workshop on LLM Evaluation</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="he2025believingseeingqualityscores" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2509.25844" target="_blank" rel="external nofollow noopener"><b>Believing without Seeing: Quality Scores for Contextualizing Vision-Language Model Explanations</b></a></div> <div class="author"> <a href="https://www.linkedin.com/in/keyu-he-569547198" rel="external nofollow noopener" target="_blank"> <b> Keyu He </b> </a>, Tejas Srinivasan, <a href="https://brihijoshi.github.io/" rel="external nofollow noopener" target="_blank"> <b> Brihi Joshi </b> </a>, Xiang Ren, Jesse Thomason, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>Under Review</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> </div> <div id="kulkarni2025evaluatingevaluationmetrics" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2504.18114" target="_blank" rel="external nofollow noopener"><b>Evaluating Evaluation Metrics – The Mirage of Hallucination Detection</b></a></div> <div class="author"> <a href="https://athrvkk.github.io/" rel="external nofollow noopener" target="_blank"> <b> Atharva Kulkarni </b> </a> , Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, and Hong Yu </div> <div class="periodical"> <em>Findings of EMNLP</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> </div> <div id="joshi2025pbnj" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2504.17993" target="_blank" rel="external nofollow noopener"><b>Improving LLM Personas via Rationalization with Psychological Scaffolds</b></a></div> <div class="author"> <a href="https://brihijoshi.github.io/" rel="external nofollow noopener" target="_blank"> <b> Brihi Joshi </b> </a>, Xiang Ren, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, Rik Koncel-Kedziorski, and Tim Paek </div> <div class="periodical"> <em>Findings of EMNLP</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Language models prompted with a user description or persona can predict a user’s preferences and opinions, but existing approaches to building personas – based solely on a user’s demographic attributes and/or prior judgments – fail to capture the underlying reasoning behind said user judgments. We introduce PB&amp;J (Psychology of Behavior and Judgments), a framework that improves LLM personas by incorporating rationales of why a user might make specific judgments. These rationales are LLM-generated, and aim to reason about a user’s behavior on the basis of their experiences, personality traits or beliefs. This is done using psychological scaffolds – structured frameworks grounded in theories such as the Big 5 Personality Traits and Primal World Beliefs – that help provide structure to the generated rationales. Experiments on public opinion and movie preference prediction tasks demonstrate that LLM personas augmented with PB&amp;J rationales consistently outperform methods using only a user’s demographics and/or judgments. Additionally, LLM personas constructed using scaffolds describing user beliefs perform competitively with those using human-written rationales.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL</abbr> </div> <div id="cui2025robustdatawatermarkinglanguage" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2503.04036" target="_blank" rel="external nofollow noopener"><b>Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge</b></a></div> <div class="author"> <a href="https://x-f-cui.github.io/website/" rel="external nofollow noopener" target="_blank"> <b> Xinyue Cui </b> </a>, Johnny Tian-Zheng Wei, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, and Robin Jia </div> <div class="periodical"> <em>Findings of ACL</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization after pretraining, while overlooking challenges that arise in other stages of the LLM pipeline, such as the risk of watermark filtering during data preprocessing, or potential forgetting through post-training, or verification difficulties due to API-only access. We propose a novel data watermarking approach that injects coherent and plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks’ density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain robust throughout LLM development, maintaining their effectiveness after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL</abbr> </div> <div id="joshi2025eliwhy" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2506.14200" target="_blank" rel="external nofollow noopener"><b>ELI-Why: Evaluating the Pedagogical Utility of LLM Explanations</b></a></div> <div class="author"> <a href="https://brihijoshi.github.io/" rel="external nofollow noopener" target="_blank"> <b> Brihi Joshi </b> </a>, <a href="https://www.linkedin.com/in/keyu-he-569547198" rel="external nofollow noopener" target="_blank"> <b> Keyu He </b> </a>, Sahana Ramnath, Sadra Sabouri, Kaitlyn Zhou, Souti Chattopadhyay, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, and Xiang Ren </div> <div class="periodical"> <em>Findings of ACL</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> <a href="https://github.com/INK-USC/ELI-Why" class="abstract btn btn-sm z-depth-1 rounded" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations’ fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/separability-480.webp 480w,/assets/img/publication_preview/separability-800.webp 800w,/assets/img/publication_preview/separability-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/separability.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="separability.jpeg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghosh2024compare" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2407.01878" target="_blank" rel="external nofollow noopener"><b>Compare without Despair: Reliable Preference Evaluation with Generation Separability</b></a></div> <div class="author"> <a href="https://sghosh73.github.io" rel="external nofollow noopener" target="_blank"> <b> Sayan Ghosh </b> </a>, Tejas Srinivasan, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>In Findings of EMNLP</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Human evaluation of generated language through pairwise preference judgments is pervasive. However, under common scenarios, such as when generations from a model pair are very similar, or when stochastic decoding results in large variations in generations, it results in inconsistent preference ratings. We address these challenges by introducing a meta-evaluation measure, separability, which estimates how suitable a test instance is for pairwise preference evaluation. For a candidate test instance, separability samples multiple generations from a pair of models, and measures how distinguishable the two sets of generations are. Our experiments show that instances with high separability values yield more consistent preference ratings from both human- and auto-raters. Further, the distribution of separability allows insights into which test benchmarks are more valuable for comparing models. Finally, we incorporate separability into ELO ratings, accounting for how suitable each test instance might be for reliably ranking LLMs. Overall, separability has implications for consistent, efficient and robust preference evaluation of LLMs with both human- and auto-raters.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/oath_frames-480.webp 480w,/assets/img/publication_preview/oath_frames-800.webp 800w,/assets/img/publication_preview/oath_frames-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/oath_frames.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="oath_frames.jpeg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ranjit2024oath" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2406.14883" target="_blank" rel="external nofollow noopener"><b>OATH-Frames: Characterizing Online Attitudes Towards Homelessness via LLM Assistants</b></a></div> <div class="author"> <a href="https://jr4fs.github.io/" rel="external nofollow noopener" target="_blank"> <b> Jaspreet Ranjit </b> </a>, <a href="https://brihijoshi.github.io/" rel="external nofollow noopener" target="_blank"> <b> Brihi Joshi </b> </a>, Rebecca Dorn, Laura Petry, Olga Koumoundouros, <a href="https://www.linkedin.com/in/jayne-bottarini/" rel="external nofollow noopener" target="_blank"> <b> Jayne Bottarini </b> </a> , Peichen Liu, Eric Rice, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>In Proceedings of EMNLP</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-1 rounded" role="button">Outstanding Paper Award @ EMNLP 2024; Best Poster @ ShowCAIS’24</a> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> <a href="https://dill-lab.github.io/oath-frames/" class="btn btn-sm z-depth-1 rounded" role="button">Blog</a> <a href="https://github.com/dill-lab/oath-frames" class="abstract btn btn-sm z-depth-1 rounded" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper Award @ EMNLP 2024; Jaspreet received a <a href="https://sites.google.com/usc.edu/showcais-2024/awards?authuser=0" rel="external nofollow noopener" target="_blank">best poster award</a> at USC CAIS’s annual symposium, <a href="https://sites.google.com/usc.edu/showcais-2024/" rel="external nofollow noopener" target="_blank">ShowCAIS</a> in Spring 2024.</p> </div> <div class="abstract hidden"> <p>Homelessness in the U.S. is widespread; individual beliefs and attitudes towards homelessness—often expressed on social media are complex and nuanced (e.g. critical as well as sympathetic). Such attitudes can be challenging to summarize at scale, obfuscating the broader public opinion which advocacy organizations use to guide public policy and reform efforts. Our work proposes an approach to enable a large-scale study on homelessness via two major contributions. First, with the help of domain experts in social work and their trainees, we characterize Online Attitudes towards Homelessness in nine hierarchical frames (OATH-Frames) on a collection of 4K social media posts. Further, in an effort to ease the annotation of these frames, we employ GPT-4 as an LLM assistant to the experts; GPT-4 + Expert annotation presents an attractive trade off owing to a 6.5× speedup in annotation time despite only incurring a 2 point F1 difference in annotation performance. Our effort results in a collection of 8K social media posts labeled by domain and trained experts (with and without GPT-4 assistance). Second, using predicted OATH-Frames on a Flan-T5-Large model trained on our data, we perform a large-scale analysis on 2.4M posts on homelessness. We find that posts that contain mentions of west coast states express more harmful generalizations of people experiencing homelessness (PEH) compared to posts about east coast states. We also find marked differences in attitudes across vulnerable populations as they are compared to PEH as being either more or less deserving of aid.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nnk-means-480.webp 480w,/assets/img/publication_preview/nnk-means-800.webp 800w,/assets/img/publication_preview/nnk-means-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/nnk-means.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nnk-means.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gulati2024out" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2407.13141" target="_blank" rel="external nofollow noopener"><b>Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression</b></a></div> <div class="author"> <a href="https://www.linkedin.com/in/aryan-gulati/" rel="external nofollow noopener" target="_blank"> <b> Aryan Gulati </b> </a>, <a href="https://www.linkedin.com/in/xingjiandong/" rel="external nofollow noopener" target="_blank"> <b> Xingjian Dong </b> </a>, Carlos Hurtado, Sarath Shekkizhar, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, and Antonio Ortega </div> <div class="periodical"> <em>In Findings of EMNLP</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>As language models become more general purpose, increased attention needs to be paid to detecting out-of-distribution (OOD) instances, i.e., those not belonging to any of the distributions seen during training. Existing methods for detecting OOD data are computationally complex and storage-intensive. We propose a novel soft clustering approach for OOD detection based on non-negative kernel regression. Our approach greatly reduces computational and space complexities (up to 11\times improvement in inference time and 87% reduction in storage requirements) and outperforms existing approaches by up to 4 AUROC points on four different benchmarks. We also introduce an entropy-constrained version of our algorithm, which leads to further reductions in storage requirements (up to 97% lower than comparable approaches) while retaining competitive performance. Our soft clustering approach for OOD detection highlights its potential for detecting tail-end phenomena in extreme-scale data settings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">COLM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/crowd-calibrator-480.webp 480w,/assets/img/publication_preview/crowd-calibrator-800.webp 800w,/assets/img/publication_preview/crowd-calibrator-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/crowd-calibrator.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="crowd-calibrator.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="khurana2024crowd" class="col-sm-8"> <div class="title">Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in Subjective Tasks?</div> <div class="author"> Urja Khurana, Eric Nalisnick, Antske Fokkens, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>In Proceedings of COLM</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Subjective tasks in NLP have been mostly relegated to objective ones where the gold label is decided by taking the majority vote, thereby obfuscating annotator disagreement and inherent uncertainty of instances. We argue that that subjectivity should play a role in model decisions, considering a selective prediction setting. However, instead of calibrating confidence purely from the model’s perspective, we calibrate models for subjective tasks based on crowdworker agreement. Our method, Crowd-Calibrator, models annotations from crowdworkers and the distance between crowdworker distribution and the model’s own distribution over labels to inform whether the model should abstain from a decision. On two highly subjective tasks, namely hate speech detection and natural language inference (NLI), our experiments show Crowd-Calibrator either outperforming or achieving competitive performance with selective prediction baselines, highlighting the value of bringing in human decision making into model predictions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">COLM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/logits-480.webp 480w,/assets/img/publication_preview/logits-800.webp 800w,/assets/img/publication_preview/logits-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/logits.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="logits.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="finlayson2024logits" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2403.09539" target="_blank" rel="external nofollow noopener"><b>Logits of API-Protected LLMs Leak Proprietary Information</b></a></div> <div class="author"> <a href="https://mattf1n.github.io/" rel="external nofollow noopener" target="_blank"> <b> Matthew Finlayson </b> </a>, Xiang Ren, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>In Proceedings of COLM</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI’s gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM’s hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI’s gpt-3.5-turbo to be about 4,096. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fn-conditioned-generation-480.webp 480w,/assets/img/publication_preview/fn-conditioned-generation-800.webp 800w,/assets/img/publication_preview/fn-conditioned-generation-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fn-conditioned-generation.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fn-conditioned-generation.jpeg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cui2024annotating" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2406.04834" target="_blank" rel="external nofollow noopener"><b>Annotating FrameNet via Structure-Conditioned Language Generation</b></a></div> <div class="author"> <a href="https://x-f-cui.github.io/website/" rel="external nofollow noopener" target="_blank"> <b> Xinyue Cui </b> </a>, and <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a> </div> <div class="periodical"> <em>In Proceedings of ACL</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> <a href="https://github.com/dill-lab/FrameNet-Conditional-Generation" class="abstract btn btn-sm z-depth-1 rounded" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Despite the mounting evidence for generative capabilities of language models in understanding and generating natural language, their effectiveness on explicit manipulation and generation of linguistic structures remain understudied. In this paper, we investigate the task of generating new sentences preserving a given semantic structure, following the FrameNet formalism. We propose a framework to produce novel frame-semantically annotated sentences following an overgenerate-and-filter approach. Our results show that conditioning on rich, explicit semantic information tends to produce generations with high human acceptance, under both prompting and finetuning. Nevertheless, we discover that generated frame-semantic structured data is ineffective at training data augmentation for frame-semantic role labeling. Our study concludes that while generating high-quality, semantically rich data might be within reach, their downstream utility remains to be seen, highlighting the outstanding challenges with automating linguistic annotation tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bat-480.webp 480w,/assets/img/publication_preview/bat-800.webp 800w,/assets/img/publication_preview/bat-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/bat.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bat.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="finlayson2023closing" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2310.01693" target="_blank" rel="external nofollow noopener"><b>Closing the Curious Case of Neural Text Degeneration</b></a></div> <div class="author"> <a href="https://mattf1n.github.io/" rel="external nofollow noopener" target="_blank"> <b> Matthew Finlayson </b> </a>, John Hewitt, Alexander Koller, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, and Ashish Sabharwal </div> <div class="periodical"> <em>In Proc. of ICLR</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> <a href="https://github.com/mattf1n/basis-aware-threshold" class="abstract btn btn-sm z-depth-1 rounded" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICASSP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/video-summarization-480.webp 480w,/assets/img/publication_preview/video-summarization-800.webp 800w,/assets/img/publication_preview/video-summarization-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/video-summarization.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="video-summarization.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nam2023does" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2309.09405" target="_blank" rel="external nofollow noopener"><b>Does Video Summarization Require Videos? Quantifying the Effectiveness of Language in Video Summarization</b></a></div> <div class="author"> <a href="https://yoonsoonam.com/" rel="external nofollow noopener" target="_blank"> <b> Yoonsoo Nam </b> </a>, <a href="https://www.linkedin.com/in/adam-lehavi/" rel="external nofollow noopener" target="_blank"> <b> Adam Lehavi </b> </a>, Daniel Yang, Digbalay Bose, <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank"> <b> Swabha Swayamdipta </b> </a>, and Shrikanth Narayanan </div> <div class="periodical"> <em>In Proc. of ICASSP</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-1 rounded" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Video summarization remains a huge challenge in computer vision due to the size of the input videos to be summarized. We propose an efficient, language-only video summarizer that achieves competitive accuracy with high data efficiency. Using only textual captions obtained via a zero-shot approach, we train a language transformer model and forego image representations. This method allows us to perform filtration amongst the representative text vectors and condense the sequence. With our approach, we gain explainability with natural language that comes easily for human interpretation and textual summaries of the videos. An ablation study that focuses on modality and data compression shows that leveraging text modality only effectively reduces input data processing while retaining comparable results.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"... and preprints",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-people",title:"people",description:"",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"nav-opportunities",title:"opportunities",description:"for prospective students in the DILL Lab",section:"Navigation",handler:()=>{window.location.href="/opportunities/"}},{id:"nav-social",title:"social",description:"DILL Lab through the years.",section:"Navigation",handler:()=>{window.location.href="/social/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-github-metadata",title:"a post with github metadata",description:"a quick run down on accessing github metadata.",section:"Posts",handler:()=>{window.location.href="/blog/2020/github-metadata/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-lt-a-href-quot-https-aclanthology-org-2024-emnlp-main-724-quot-gt-oath-frames-lt-a-gt-wins-an-outstanding-paper-award-at-emnlp-2024",title:"&lt;a href=&quot;https://aclanthology.org/2024.emnlp-main.724/&quot;&gt;OATH-Frames&lt;/a&gt; wins an Outstanding Paper Award at EMNLP 2024!",description:"",section:"News"},{id:"news-dill-has-3-acceptances-to-emnlp-2024-congrats-to-jaspreet-and-brihi-for-lt-a-href-quot-https-arxiv-org-abs-2406-14883-quot-gt-oath-frames-lt-a-gt-sayan-for-lt-a-href-quot-https-arxiv-org-abs-2407-01878-quot-gt-separability-lt-a-gt-and-aryan-and-danny-for-lt-a-href-quot-https-arxiv-org-abs-2407-13141-quot-gt-ood-detection-with-nnk-means-lt-a-gt",title:"DILL has 3 acceptances to EMNLP 2024! Congrats to Jaspreet and Brihi for &lt;a href=&quot;https://arxiv.org/abs/2406.14883&quot;&gt;OATH frames&lt;/a&gt;, Sayan for &lt;a href=&quot;https://arxiv.org/abs/2407.01878&quot;&gt;Separability&lt;/a&gt;, and Aryan and Danny for &lt;a href=&quot;https://arxiv.org/abs/2407.13141&quot;&gt;OOD detection with NNK-means&lt;/a&gt;!",description:"",section:"News"},{id:"news-welcome-to-our-new-postdoc-lt-a-href-quot-https-gyauney-github-io-quot-gt-greg-yauney-lt-a-gt",title:"Welcome to our new postdoc: &lt;a href=&quot;https://gyauney.github.io/&quot;&gt;Greg Yauney&lt;/a&gt;!",description:"",section:"News"},{id:"news-jaspreet-and-brihi-s-work-on-homelessness-and-oath-frames-gets-some-lt-a-href-quot-https-viterbischool-usc-edu-news-2024-09-ai-solutions-for-social-good-ph-d-student-enlists-llm-assistants-on-project-addressing-homelessness-quot-gt-usc-media-coverage-lt-a-gt",title:"Jaspreet and Brihi\u2019s work on homelessness and OATH frames gets some &lt;a href=&quot;https://viterbischool.usc.edu/news/2024/09/ai-solutions-for-social-good-ph-d-student-enlists-llm-assistants-on-project-addressing-homelessness/&quot;&gt;USC media coverage&lt;/a&gt;.",description:"",section:"News"},{id:"news-welcome-to-our-3-new-phd-students-in-the-dill-lab-xinyue-cui-atharva-kulkarni-and-muru-zhang",title:"Welcome to our 3 new PhD students in the DILL Lab: Xinyue Cui, Atharva Kulkarni and Muru Zhang.",description:"",section:"News"},{id:"news-matt-s-paper-on-lt-a-href-quot-https-arxiv-org-abs-2403-09539-quot-gt-logits-of-api-protected-llms-leak-proprietary-information-lt-a-gt-and-urja-s-paper-on-lt-a-href-quot-quot-gt-crowd-calibrator-can-annotator-disagreement-inform-calibration-in-subjective-tasks-lt-a-gt-both-accepted-to-colm-24",title:"Matt\u2019s paper on &lt;a href=&quot;https://arxiv.org/abs/2403.09539&quot;&gt;Logits of API-Protected LLMs Leak Proprietary Information&lt;/a&gt; and Urja\u2019s paper on &lt;a href=&quot;&quot;&gt;Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in Subjective Tasks?&lt;/a&gt; both accepted to COLM\u201924! \ud83c\udf89",description:"",section:"News"},{id:"news-dill-lab-submits-3-papers-to-emnlp-preprints-out-soon",title:"DILL Lab submits 3 papers to EMNLP. Preprints out soon!",description:"",section:"News"},{id:"news-xinyue-s-paper-on-lt-a-href-quot-https-arxiv-org-abs-2406-04834-quot-gt-structure-conditioned-generation-with-framenet-lt-a-gt-gets-accepted-to-acl-24",title:"Xinyue\u2019s paper on &lt;a href=&quot;https://arxiv.org/abs/2406.04834&quot;&gt;Structure-Conditioned Generation with FrameNet&lt;/a&gt; gets accepted to ACL\u201924! \ud83c\udf89",description:"",section:"News"},{id:"news-jaspreet-received-a-best-poster-award-at-lt-a-href-quot-https-sites-google-com-usc-edu-showcais-2024-home-authuser-0-quot-gt-showcais-2024-lt-a-gt-for-her-work-on-lt-a-href-quot-https-dill-lab-github-io-oath-frames-quot-gt-oath-frames-lt-a-gt",title:"Jaspreet received a best poster award at &lt;a href=&quot;https://sites.google.com/usc.edu/showcais-2024/home?authuser=0&quot;&gt;ShowCAIS 2024&lt;/a&gt; for her work on &lt;a href=&quot;https://dill-lab.github.io/oath-frames/&quot;&gt;OATH Frames&lt;/a&gt;. \ud83c\udf89",description:"",section:"News"},{id:"news-dill-lab-submits-2-papers-to-colm-24",title:"DILL lab submits 2 papers to COLM\u201924.",description:"",section:"News"},{id:"news-dill-hosts-lt-a-href-quot-https-yanaiela-github-io-quot-gt-yanai-elazar-lt-a-gt-yi-at-the-allen-institute-for-ai",title:"DILL hosts &lt;a href=&quot;https://yanaiela.github.io/&quot;&gt;Yanai Elazar&lt;/a&gt;, YI at the Allen Institute for AI.",description:"",section:"News"},{id:"news-dill-lab-submits-three-papers-to-acl-24",title:"DILL Lab submits three papers to ACL\u201924.",description:"",section:"News"},{id:"news-matt-gave-an-invited-talk-at-cmu-lti",title:"Matt gave an invited talk at CMU LTI.",description:"",section:"News"},{id:"news-matt-s-lt-a-href-quot-https-arxiv-org-abs-2310-01693-quot-gt-paper-on-the-softmax-bottleneck-lt-a-gt-gets-accepted-to-iclr-24",title:"Matt\u2019s &lt;a href=&quot;https://arxiv.org/abs/2310.01693&quot;&gt;paper on the softmax bottleneck&lt;/a&gt; gets accepted to ICLR\u201924. \ud83c\udf89",description:"",section:"News"},{id:"news-yoonsoo-s-lt-a-href-quot-https-arxiv-org-abs-2309-09405-quot-gt-paper-on-video-summarization-lt-a-gt-now-accepted-to-icassp-24",title:"Yoonsoo\u2019s &lt;a href=&quot;https://arxiv.org/abs/2309.09405&quot;&gt;paper on video summarization&lt;/a&gt; now accepted to ICASSP\u201924. \ud83c\udf89",description:"",section:"News"},{id:"news-we-hosted-lt-a-href-quot-https-kawine-github-io-quot-gt-kawin-ethayarajh-lt-a-gt-a-phd-student-at-stanford",title:"We hosted &lt;a href=&quot;https://kawine.github.io/&quot;&gt;Kawin Ethayarajh&lt;/a&gt;, a PhD student at Stanford.",description:"",section:"News"},{id:"news-the-dill-lab-had-a-pre-thanksgiving-get-together-with-a-dinner-potluck",title:"The DILL lab had a pre-Thanksgiving get-together with a dinner potluck.",description:"",section:"News"},{id:"news-attended-lt-a-href-quot-https-socalnlp-github-io-symp23-index-html-quot-gt-socalnlp-2023-lt-a-gt-where-members-of-the-dill-lab-presented-4-papers",title:"Attended &lt;a href=&quot;https://socalnlp.github.io/symp23/index.html&quot;&gt;SoCalNLP 2023&lt;/a&gt;, where members of the DILL lab presented 4 papers.",description:"",section:"News"},{id:"news-we-hosted-lt-a-href-quot-https-sarahwie-github-io-quot-gt-sarah-wiegreffe-lt-a-gt-a-postdoctoral-researcher-from-ai2",title:"We hosted &lt;a href=&quot;https://sarahwie.github.io/&quot;&gt;Sarah Wiegreffe&lt;/a&gt;, a postdoctoral researcher from AI2.",description:"",section:"News"},{id:"news-two-new-preprints-on-arxiv-on-lt-a-href-quot-https-arxiv-org-abs-2309-09405-quot-gt-video-summarization-lt-a-gt-and-on-the-lt-a-href-quot-https-arxiv-org-abs-2310-01693-quot-gt-softmax-bottleneck-lt-a-gt",title:"Two new preprints on arxiv: on &lt;a href=&quot;https://arxiv.org/abs/2309.09405&quot;&gt;video summarization&lt;/a&gt; and on the &lt;a href=&quot;https://arxiv.org/abs/2310.01693&quot;&gt;softmax bottleneck&lt;/a&gt;.",description:"",section:"News"},{id:"news-we-hosted-lt-a-href-quot-https-juliamendelsohn-github-io-quot-gt-julia-mendelsohn-lt-a-gt-a-phd-student-from-umich-who-spoke-about-her-work-on-computational-analysis-of-nuanced-political-rhetoric",title:"We hosted &lt;a href=&quot;https://juliamendelsohn.github.io/&quot;&gt;Julia Mendelsohn&lt;/a&gt;, a PhD student from UMich who spoke about her work on Computational Analysis of Nuanced Political Rhetoric.",description:"",section:"News"},{id:"news-we-had-a-summer-ice-cream-social-at-culver-city-downtown-with-many-new-members",title:"We had a summer ice cream social at Culver City Downtown with many new members.",description:"",section:"News"},{id:"news-lt-a-href-quot-https-urjakh-github-io-quot-gt-urja-khurana-lt-a-gt-a-phd-student-from-vrije-universiteit-amsterdam-is-visiting-our-lab-this-summer-working-on-hate-speech-detection",title:"&lt;a href=&quot;https://urjakh.github.io/&quot;&gt;Urja Khurana&lt;/a&gt;, a Phd Student from Vrije Universiteit, Amsterdam is visiting our lab this summer, working on hate speech detection.",description:"",section:"News"},{id:"news-we-had-a-summer-barbecue-social-along-with-the-lt-a-href-quot-https-glamor-usc-github-io-quot-gt-glamor-lt-a-gt-lab-at-the-kenneth-hahn-state-park",title:"We had a summer barbecue social along with the &lt;a href=&quot;https://glamor-usc.github.io/&quot;&gt;GLAMOR&lt;/a&gt; lab at the Kenneth Hahn State Park.",description:"",section:"News"},{id:"news-we-hosted-incoming-assistant-professor-at-iisc-bangalore-lt-a-href-quot-https-danishpruthi-com-quot-gt-danish-pruthi-lt-a-gt-in-our-lab",title:"We hosted (incoming) Assistant Professor at IISc Bangalore, &lt;a href=&quot;https://danishpruthi.com/&quot;&gt;Danish Pruthi&lt;/a&gt; in our lab.",description:"",section:"News"},{id:"news-sayan-and-jaspreet-hosted-the-first-dill-lab-office-hours-for-usc-undergrads-interested-in-research",title:"Sayan and Jaspreet hosted the first DILL Lab Office hours for USC undergrads interested in research.",description:"",section:"News"},{id:"news-friend-of-the-lab-lt-a-href-quot-https-suchin-io-quot-gt-suchin-gururangan-lt-a-gt-gave-an-invited-talk-at-the-group-meeting",title:"Friend of the lab, &lt;a href=&quot;https://suchin.io/&quot;&gt;Suchin Gururangan&lt;/a&gt; gave an invited talk at the group meeting.",description:"",section:"News"},{id:"news-dill-attended-the-acm-undergrad-research-event-to-reach-out-to-undergrads-and-masters-students-interested-in-the-lab",title:"DILL attended the ACM Undergrad Research Event to reach out to undergrads and masters students interested in the lab.",description:"",section:"News"},{id:"news-new-usc-phd-student-lt-a-href-quot-https-scholar-google-com-citations-user-rm4hgn8aaaaj-amp-amp-hl-en-quot-gt-jiarui-zhang-lt-a-gt-gave-an-invited-talk-at-the-group-meeting",title:"New USC PhD student &lt;a href=&quot;https://scholar.google.com/citations?user=rM4hgN8AAAAJ&amp;amp;hl=en&quot;&gt;Jiarui Zhang&lt;/a&gt; gave an invited talk at the group meeting.",description:"",section:"News"},{id:"news-friend-of-the-lab-lt-a-href-quot-https-eunicemjun-com-quot-gt-eunice-jun-lt-a-gt-gave-an-invited-talk-at-the-group-meeting",title:"Friend of the lab, &lt;a href=&quot;https://eunicemjun.com/&quot;&gt;Eunice Jun&lt;/a&gt; gave an invited talk at the group meeting.",description:"",section:"News"},{id:"news-warm-welcome-to-our-latest-phd-student-lt-a-href-quot-https-brihijoshi-github-io-quot-gt-brihi-joshi-lt-a-gt-now-both-at-ink-and-dill-labs",title:"Warm welcome to our latest PhD student, &lt;a href=&quot;https://brihijoshi.github.io/&quot;&gt;Brihi Joshi&lt;/a&gt;, now both at INK and DILL labs.",description:"",section:"News"},{id:"news-software-engineering-phd-student-lt-a-href-quot-https-toorajhelmi-github-io-home-quot-gt-tooraj-helmi-lt-a-gt-will-be-a-guest-at-the-dill-for-the-spring-semester",title:"Software engineering PhD student &lt;a href=&quot;https://toorajhelmi.github.io/home/&quot;&gt;Tooraj Helmi&lt;/a&gt; will be a guest at the DILL for the spring semester.",description:"",section:"News"},{id:"news-dill-celebrated-thanksgiving-together-with-a-couple-of-guests-ali-omrani-and-souti-chattopadhyay-at-swabha-s",title:"DILL celebrated Thanksgiving together with a couple of guests (Ali Omrani and Souti Chattopadhyay) at Swabha\u2019s.",description:"",section:"News"},{id:"news-sayan-and-jaspreet-presented-a-brief-overview-of-their-research-projects-at-the-lt-a-href-quot-https-nlp-usc-edu-quot-gt-usc-nlp-lt-a-gt-lunch",title:"Sayan and Jaspreet presented a brief overview of their research projects at the &lt;a href=&quot;https://nlp.usc.edu/&quot;&gt;USC-NLP&lt;/a&gt; lunch.",description:"",section:"News"},{id:"news-we-attended-the-socalnlp-symposium-where-sayan-and-jaspreet-presented-their-latest-research-posters-and-swabha-gave-an-invited-talk",title:"We attended the SoCalNLP Symposium where Sayan and Jaspreet presented their latest research posters, and Swabha gave an invited talk.",description:"",section:"News"},{id:"news-first-day-at-usc-for-the-entire-dill-lab",title:"First day at USC for the entire DILL Lab!",description:"",section:"News"},{id:"news-we-now-have-four-new-lab-members",title:"We now have four new lab members!",description:"",section:"News"},{id:"projects-adam-lehavi",title:"Adam Lehavi",description:"Spring-Summer 2023",section:"Projects",handler:()=>{window.location.href="/projects/alum/adam/"}},{id:"projects-aryan-gulati",title:"Aryan Gulati",description:"Summer 2023-5 \u27a1\ufe0f Bloomberg",section:"Projects",handler:()=>{window.location.href="/projects/alum/aryan/"}},{id:"projects-catherine-he",title:"Catherine He",description:"Fall 2024",section:"Projects",handler:()=>{window.location.href="/projects/alum/catherine/"}},{id:"projects-jacky-mo",title:"Jacky Mo",description:"Fall 23 - Spring 24 \u27a1\ufe0f   PhD, UC Davis",section:"Projects",handler:()=>{window.location.href="/projects/alum/jacky/"}},{id:"projects-jayne-bottarini",title:"Jayne Bottarini",description:"Summer - Fall 2023",section:"Projects",handler:()=>{window.location.href="/projects/alum/jayne/"}},{id:"projects-joseph-liu",title:"Joseph Liu",description:"Spring 2024-5 \u27a1\ufe0f  CMU M.S.",section:"Projects",handler:()=>{window.location.href="/projects/alum/joseph/"}},{id:"projects-keyu-he",title:"Keyu He",description:"Spring 2024-5 \u27a1\ufe0f  CMU M.S.",section:"Projects",handler:()=>{window.location.href="/projects/alum/keyu/"}},{id:"projects-kritin-dhoka",title:"Kritin Dhoka",description:"Summer-Fall 2023",section:"Projects",handler:()=>{window.location.href="/projects/alum/kritin/"}},{id:"projects-ruyuan-zuo",title:"Ruyuan Zuo",description:"Summer 2023 \u27a1\ufe0f Google",section:"Projects",handler:()=>{window.location.href="/projects/alum/ruyuan/"}},{id:"projects-ryan-wang",title:"Ryan Wang",description:"Spring 2024-5 \u27a1\ufe0f PhD at UC Berkeley",section:"Projects",handler:()=>{window.location.href="/projects/alum/ryan/"}},{id:"projects-shauryasikt-jena",title:"Shauryasikt Jena",description:"Spring 2024 -",section:"Projects",handler:()=>{window.location.href="/projects/alum/shaurya/"}},{id:"projects-urja-khurana",title:"Urja Khurana",description:"Visiting PhD, Vrije Universiteit Amsterdam. Summer 2023",section:"Projects",handler:()=>{window.location.href="/projects/alum/urja/"}},{id:"projects-yoonsoo-nam",title:"Yoonsoo Nam",description:"Spring 2023 - Spring 2024 (MS) \u27a1\ufe0f RenderWolf",section:"Projects",handler:()=>{window.location.href="/projects/alum/yoonsoo/"}},{id:"projects-danny-dong",title:"Danny Dong",description:"Spring 2024 -",section:"Projects",handler:()=>{window.location.href="/projects/masters/danny/"}},{id:"projects-dhruv-tarsadiya",title:"Dhruv Tarsadiya",description:"Spring 2025 -",section:"Projects",handler:()=>{window.location.href="/projects/masters/dhruv/"}},{id:"projects-harshavardhan-alimi",title:"Harshavardhan Alimi",description:"Summer 2025 -",section:"Projects",handler:()=>{window.location.href="/projects/masters/harsha/"}},{id:"projects-shahzaib-saqib-warraich",title:"Shahzaib Saqib Warraich",description:"Spring 2024 -",section:"Projects",handler:()=>{window.location.href="/projects/masters/shahzaib/"}},{id:"projects-atharva-kulkarni",title:"Atharva Kulkarni",description:"Fall 2024 -",section:"Projects",handler:()=>{window.location.href="/projects/phd/atharva/"}},{id:"projects-brihi-joshi",title:"Brihi Joshi",description:"Fall 2021 -",section:"Projects",handler:()=>{window.location.href="/projects/phd/brihi/"}},{id:"projects-jaspreet-ranjit",title:"Jaspreet Ranjit",description:"Fall 2022 -",section:"Projects",handler:()=>{window.location.href="/projects/phd/jaspreet/"}},{id:"projects-matt-finlayson",title:"Matt Finlayson",description:"Fall 2023 -",section:"Projects",handler:()=>{window.location.href="/projects/phd/matt/"}},{id:"projects-muru-zhang",title:"Muru Zhang",description:"Fall 2024 -",section:"Projects",handler:()=>{window.location.href="/projects/phd/muru/"}},{id:"projects-sayan-ghosh",title:"Sayan Ghosh",description:"Fall 2022 -",section:"Projects",handler:()=>{window.location.href="/projects/phd/sayan/"}},{id:"projects-xinyue-cui",title:"Xinyue Cui",description:"MS Spring 2023-2024 / PhD Fall 2024 -",section:"Projects",handler:()=>{window.location.href="/projects/phd/xinyue/"}},{id:"projects-greg-yauney",title:"Greg Yauney",description:"Fall 2024 - 2025",section:"Projects",handler:()=>{window.location.href="/projects/postdocs/greg/"}},{id:"projects-swabha-swayamdipta",title:"Swabha Swayamdipta",description:"",section:"Projects",handler:()=>{window.location.href="/projects/swabha/"}},{id:"projects-brennen-ho",title:"Brennen Ho",description:"Fall 2025 -",section:"Projects",handler:()=>{window.location.href="/projects/undergrads/brennen/"}},{id:"projects-naysa-bhargava",title:"Naysa Bhargava",description:"Fall 2025 -",section:"Projects",handler:()=>{window.location.href="/projects/undergrads/naysa/"}},{id:"projects-risha-surana",title:"Risha Surana",description:"Fall 2023 -",section:"Projects",handler:()=>{window.location.href="/projects/undergrads/risha/"}},{id:"projects-thor-christoffersen-hochman",title:"Thor Christoffersen Hochman",description:"Fall 2025 -",section:"Projects",handler:()=>{window.location.href="/projects/undergrads/thor/"}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/dill-lab","_blank")}},{id:"socials-discord",title:"Discord",section:"Socials",handler:()=>{window.open("https://discord.com/users/1245805520120451154","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>
---
layout: about
title: about
permalink: /
subtitle: <a href='https://www.usc.edu/'>University of Southern California</a> •  <a href='https://www.cs.usc.edu/'>Viterbi CS</a> •  <a href='https://www.nlp.usc.edu/'>USC NLP</a>

profile:
  align: right
  image: dill-canva-transp.png
  image_circular: false # crops the image to make it circular
  more_info: RTH 420

news: true # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

The _**D**ata_, _**I**nterpretability_, _**L**anguage_ and _**L**earning_, (**DILL**) lab, led by [Swabha Swayamdipta](https://swabhs.com), explores questions in the intersection of natural language processing and machine learning.
<!-- DILL is focused on automatically estimating the quality of datasets for models, efficient pretraining, and semi-automatically building datasets that help models learn better. -->
<!-- We also emphasize on special aspects of language that affect dataset quality, such as subjectivity and ambiguity. -->
Our research considers language (and other) data, models that train on this data and their impact on society.
These are some questions we are currently exploring:

- How can we rigorously evaluate the generative capabilities of language models?
- Do language models have the ability to generate text with specific properties and what does that reveal about them?
- What roles do different kinds of data play in the successes or failures of language models?
- How can we use language models to understand our society better?


Check out our [latest publications](/publications/) and a [short summary of the questions we are currently working on](/opportunities).
